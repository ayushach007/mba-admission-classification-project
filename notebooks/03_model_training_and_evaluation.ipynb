{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from src.logger import logging  \n",
    "from src.exception import CustomException\n",
    "from src import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from src.utils.common import read_yaml_file, save_object, eval_model, save_model_metrics, create_directory\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    model_path: Path\n",
    "    training_metrics: Path\n",
    "    test_metrics: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                  config_file_path = CONFIG_FILE_PATH):\n",
    "        try:\n",
    "            self.config = read_yaml_file(config_file_path)\n",
    "\n",
    "            logging.info(\"Configuration and Parameters files have been read successfully\")\n",
    "\n",
    "            logging.info(\"Creating directories to store artifacts\")\n",
    "            create_directory([self.config.artifacts_directory])\n",
    "            logging.info(\"Directories have been created successfully\")\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def get_model_config(self) -> ModelTrainingConfig:\n",
    "        try:\n",
    "            config = self.config.model_training\n",
    "            logging.info(\"Creating directories to store model artifacts\")\n",
    "            create_directory([config.root_dir])\n",
    "\n",
    "            logging.info(\"Directories have been created successfully\")\n",
    "\n",
    "            logging.info(\"Assigning paths to model and model metrics\")\n",
    "\n",
    "            model_config = ModelTrainingConfig(\n",
    "                model_path = config.model_path,\n",
    "                training_metrics = config.training_metrics,\n",
    "                test_metrics = config.test_metrics,\n",
    "                )\n",
    "            \n",
    "            logging.info(\"Paths have been assigned successfully\")\n",
    "\n",
    "            return model_config\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilding:\n",
    "    '''\n",
    "    This class is responsible for building models, hyperparameter tuning, training and evaluating models, saving models and metrics, and saving the best model\n",
    "    '''\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def initiate_model_building(self, train_arr, test_arr):\n",
    "        '''\n",
    "        This function initiates model building process, splits data into train and test sets, specifies models to be trained, hyperparameter tuning for models, model training and evaluation, saves model and metrics, and saves the best model\n",
    "        \n",
    "        Args:\n",
    "            - train_arr: Training data\n",
    "            - test_arr: Testing data\n",
    "            \n",
    "        Returns:\n",
    "            - best_model: Best model\n",
    "            - best_model_score: Best model score\n",
    "            \n",
    "        Raises:\n",
    "            - CustomException: If any error occurs while initiating model building or saving the best model\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            logging.info(\"Initiating model building process\")\n",
    "            logging.info(\"Splitting data into train and test sets\")\n",
    "            X_train = train_arr[:, :-1]\n",
    "            y_train = train_arr[:, -1]\n",
    "            X_test = test_arr[:, :-1]\n",
    "            y_test = test_arr[:, -1]\n",
    "\n",
    "\n",
    "            # doing oversampling as the data is imbalanced\n",
    "            sm = SMOTE(random_state=42)\n",
    "            logging.info(\"Oversampling the data\")\n",
    "\n",
    "            X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "            logging.info(\"Oversampling has been done successfully\")\n",
    "\n",
    "            logging.info(\"Splitting has been done successfully\")\n",
    "\n",
    "            logging.info(\"Specifying models to be trained\")\n",
    "\n",
    "            models = {\n",
    "                \"LogisticRegression\": LogisticRegression(),\n",
    "                \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "                \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "                \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "                \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "                \"SVC\": SVC(),\n",
    "                \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "                \"XGBClassifier\": XGBClassifier()\n",
    "            }\n",
    "\n",
    "            logging.info(\"Models have been specified successfully\")\n",
    "\n",
    "            logging.info(\"Hyperparameter tuning for models\")\n",
    "\n",
    "            params = {\n",
    "                \"LogisticRegression\": {\n",
    "                    \"C\": [0.01, 0.1, 1]\n",
    "                },\n",
    "\n",
    "                \"RandomForestClassifier\": {\n",
    "                    \"n_estimators\": [100, 200, 300],\n",
    "                    \"max_depth\": [5, 10, 15, 20]\n",
    "                },  \n",
    "\n",
    "                \"DecisionTreeClassifier\": {\n",
    "                    \"max_depth\": [5, 10, 20, 30],\n",
    "                    \"min_samples_split\": [ 5, 10, 15]\n",
    "                },\n",
    "\n",
    "                \"GradientBoostingClassifier\": {\n",
    "                    \"n_estimators\": [100, 200, 300],\n",
    "                    \"learning_rate\": [0.01, 0.1, 1]\n",
    "                },\n",
    "\n",
    "                \"AdaBoostClassifier\": {\n",
    "                    \"n_estimators\": [50, 100, 200, 300],\n",
    "                    \"learning_rate\": [0.01, 0.1, 1]\n",
    "                },\n",
    "\n",
    "                \"SVC\": {\n",
    "                    \"C\": [0.01, 0.1, 1],\n",
    "                    \"degree\": [3, 4, 5]\n",
    "                },\n",
    "\n",
    "                \"KNeighborsClassifier\": {\n",
    "                    \"n_neighbors\": [3, 5, 7],\n",
    "                    \"weights\": ['uniform', 'distance']\n",
    "                },\n",
    "\n",
    "                \"XGBClassifier\": {\n",
    "                    \"n_estimators\": [100, 200, 300],\n",
    "                    \"learning_rate\": [0.01, 0.1, 1]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            logging.info(\"Hyperparameter tuning has been done successfully\")\n",
    "\n",
    "            logging.info(\"Model training and evaluation\")\n",
    "\n",
    "            # Evaluate models\n",
    "            training_metrics, test_metrics = eval_model(X_train, X_test, y_train, y_test, models, params)\n",
    "\n",
    "            logging.info(\"Model training and evaluation has been done successfully\")\n",
    "\n",
    "            logging.info(\"Saving model and metrics\")\n",
    "            \n",
    "            # Save model metrics\n",
    "            save_model_metrics(\n",
    "                report = training_metrics,\n",
    "                path = self.config.training_metrics\n",
    "            )\n",
    "\n",
    "            save_model_metrics(\n",
    "                report = test_metrics,\n",
    "                path = self.config.test_metrics\n",
    "            )\n",
    "\n",
    "            \n",
    "            logging.info(\"Model and metrics have been saved successfully\")\n",
    "\n",
    "            # Select the best model based on test accuracy\n",
    "            best_model_score = max(test_metrics.values(), key=lambda x: x['accuracy'])['accuracy']\n",
    "            best_model_name = max(test_metrics, key=lambda x: test_metrics[x]['accuracy'])\n",
    "\n",
    "            best_model = models[best_model_name]\n",
    "\n",
    "            if best_model_score < 0.75:\n",
    "                logging.warning(\"Model performance is below 75%. Please consider retraining the model\")\n",
    "\n",
    "            logging.info(f'The best model is {best_model_name} with an accuracy score of {best_model_score}')\n",
    "\n",
    "            logging.info(\"Saving the best model\")\n",
    "            save_object(\n",
    "                object = best_model,\n",
    "                object_path = self.config.model_path\n",
    "            )\n",
    "\n",
    "            logging.info(\"Model has been saved successfully\")\n",
    "\n",
    "            return best_model, best_model_score\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.data_ingestion import DataIngestion, DataIngestionConfig\n",
    "from src.components.data_transformation import DataTransformation, DataTransformationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        config = ConfigurationManager()\n",
    "        model_config = config.get_model_config()\n",
    "\n",
    "        training = ModelBuilding(model_config)\n",
    "        training.initiate_model_building(\n",
    "            train_arr= 'artifacts/data_transformation/train_arr.csv'\n",
    "            test_arr= 'artifacts/data_transformation/test_arr.csv'\n",
    "    )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        raise CustomException(e, sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "admission",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
